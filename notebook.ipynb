{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hydrogeodesy: Monitoring surface waters from space\n",
    "### Exercise 2: Comparison of different inland altimetry products\n",
    "\n",
    "Daniel Scherer, DGFI-TUM  \n",
    "Wintersemester 2022/23\n",
    "\n",
    "**Contents**\n",
    "1. Reading NetCDF formatted data\n",
    "2. Data processing\n",
    "3. Quality assessment\n",
    "\n",
    "**Study Area: Lake Erie (USA)**  \n",
    "![AOI](aoi.Png)  \n",
    "*Figure 1: Lake Erie with intersecting satellite altimetry passes. Red: Topex/Poseidon and Jason-1/-2 missions. Blue: ERS-2 and Envisat missions*  \n",
    "\n",
    "Water Level Variation: 173 – 176 m\n",
    "\n",
    "Following data is used in this exercise:  \n",
    "\n",
    "| Dataset | Institution | Epoche | Samples | Filename |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| [In-Situ](https://tidesandcurrents.noaa.gov/waterlevels.html?id=9063053) | NOAA | 1992-2015 | 8489 (daily) | gauge_33.nc |\n",
    "| [DAHITI](https://dahiti.dgfi.tum.de/en/6/water-level-altimetry/) | DGFI-TUM | 1992-2017 | 2289 | dahiti_6.nc |\n",
    "| [Hydroweb](https://hydroweb.theia-land.fr/hydroweb/view/L_erie?lang=en) | LEGOS | 1992-2011 | 222 | hydroweb_1560.nc |\n",
    "| [River&Lake](http://altimetry.esa.int/riverlake/shared/main.html) | ESA | 2002-2010 | 81 | Esa_778.nc |\n",
    "| [GRLM](https://ipad.fas.usda.gov/cropexplorer/global_reservoir/gr_regional_chart.aspx?regionid=us&reservoir_name=Erie) | USDA | 1992-2014 | 752 | grlm_75.nc |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Reading NetCDF formatted data\n",
    "Use the netcdf4 package to read NetCDF data in python.  \n",
    "e.g. the DAHITI data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "dset = nc.Dataset('data/dahiti_6.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get information about the data set by just printing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And inspect the variables and dimensions by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dset.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access a variable as shown, it will return a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights_dahiti = dset['height'][:]\n",
    "print(type(heights_dahiti))\n",
    "print(heights_dahiti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good practice to eventually close the dataset to prevent memory leaks and permission issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the water level data from the files in the *data* folder.  \n",
    "To make the following steps easier, we create a ***[Pandas Series](https://pandas.pydata.org/docs/reference/api/pandas.Series.html)*** object for each source.  \n",
    "The data of the series should be the water level and the index should be the respective julian days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dahiti = pd.Series(data = ..., index = ...)\n",
    "\n",
    "esa = ...\n",
    "\n",
    "gauge = ...\n",
    "\n",
    "grlm = ...\n",
    "# Somehow the GRLM data contains duplicated entries but we keep only the first:\n",
    "grlm = grlm[~grlm.index.duplicated(keep='first')]\n",
    "\n",
    "hw = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "dahiti.plot(ax=ax,color=\"blue\",legend=True,label='DAHITI')\n",
    "esa.plot(ax=ax,color=\"green\",legend=True,label='ESA')\n",
    "gauge.plot(ax=ax,color=\"black\",legend=True,label='Gauge')\n",
    "grlm.plot(ax=ax,color=\"red\",legend=True,label='GRLM')\n",
    "hw.plot(ax=ax,color=\"orange\",legend=True,label='HydroWeb')\n",
    "plt.ylabel(\"Water Level [m]\")\n",
    "plt.xlabel(\"Julian Days\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are significant offsets between the time series, as they relate to different references.  \n",
    "We will fix this in the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Processing\n",
    "\n",
    "Prepare the data to compare each altimetry dataset (dahiti, hydroweb, esa, grlm) with the in-situ gauge data.\n",
    "\n",
    "There may be an offset between both datasets, e.g. because of different vertical datums, which should be removed.\n",
    "\n",
    "We do ot need to find intersecting days of observations in advance, because we use the pandas series objects.  \n",
    "The index (julian days) is automatically compared during the operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_dahiti = (gauge - dahiti).median()\n",
    "dahiti += offset_dahiti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and apply the offset for each dataset and plot the data as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_hw = ...\n",
    "hw += ...\n",
    "offset_esa = ...\n",
    "esa += ...\n",
    "offset_grlm = ...\n",
    "grlm += ...\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "dahiti.plot(ax=ax,color=\"blue\",legend=True,label='DAHITI')\n",
    "esa.plot(ax=ax,color=\"green\",legend=True,label='ESA')\n",
    "gauge.plot(ax=ax,color=\"black\",legend=True,label='Gauge')\n",
    "grlm.plot(ax=ax,color=\"red\",legend=True,label='GRLM')\n",
    "hw.plot(ax=ax,color=\"orange\",legend=True,label='HydroWeb')\n",
    "plt.ylabel(\"Water Level [m]\")\n",
    "plt.xlabel(\"Julian Days\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality Assessment\n",
    "\n",
    "Build functions to calculate the **Root Mean Square Error (RMSE), Nash-Sutcliff Efficiency (NSE), and squared Pearson's correlation coefficient (R²)** between both datasets.\n",
    "\n",
    "The formulas are given in the lecture slides \"Satellite Altimetry (Part 2)\" pp. 47ff.\n",
    "\n",
    "You can either use pandas buildin functions (*i.e. (series_a - series_b).mean()*) or numpy on the intersection of both series:\n",
    "\n",
    "    idx = series_a.index.intersection(series_b.index)\n",
    "    array_a, array_b = series_a[idx].to_numpy(), series_b[idx].to_numpy()\n",
    "    \n",
    "Useful numpy functions are:\n",
    "- np.power(*base, exponent*)\n",
    "- np.sum(*values*)\n",
    "- np.cov(*values_a, values_b*)\n",
    "- np.std(*values*)\n",
    "- np.mean(*values*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rmse(a, b):\n",
    "    return ...\n",
    "\n",
    "def corr(a, b):\n",
    "    return ...\n",
    "\n",
    "def nse(a,b):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the functions to compare each dataset with insitu.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_dahiti = rmse(dahiti,gauge)\n",
    "corr_dahiti = corr(dahiti,gauge)\n",
    "nse_dahiti = nse(dahiti,gauge)\n",
    "\n",
    "rmse_hw = rmse(hw,gauge)\n",
    "corr_hw = corr(hw,gauge)\n",
    "nse_hw = nse(hw,gauge)\n",
    "\n",
    "rmse_esa = rmse(esa,gauge)\n",
    "corr_esa = corr(esa,gauge)\n",
    "nse_esa = nse(esa,gauge)\n",
    "\n",
    "rmse_grlm = rmse(grlm,gauge)\n",
    "corr_grlm = corr(grlm,gauge)\n",
    "nse_grlm = nse(grlm,gauge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Quality Assessment\n",
    "Display a summary of the quality measures.\n",
    "Interpret and discuss the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame.from_dict({\n",
    "    \"Agency\": [\"DGFI-TUM\",\"LEGOS\",\"ESA\",\"USDA\"],\n",
    "    \"Name\" : ['DAHITI','Hydroweb',\"Rivers & Lakes\",'GRLM'],\n",
    "    \"Offset [m]\" : np.round([offset_dahiti, offset_hw, offset_esa, offset_grlm],3),\n",
    "    'RMSE [m]' : np.round([rmse_dahiti, rmse_hw, rmse_esa, rmse_grlm],3),\n",
    "    'Corr.' : np.round([corr_dahiti, corr_hw, corr_esa, corr_grlm],3),\n",
    "    'NSE' : np.round([nse_dahiti, nse_hw, nse_esa, nse_grlm],3)\n",
    "},orient='index')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
